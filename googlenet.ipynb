{"cells":[{"cell_type":"code","execution_count":null,"id":"be115f11","metadata":{"id":"be115f11"},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","\n","# Define the path to the root directory where your data is stored\n","data_root = '/home/admin1/Downloads/small datast'\n","\n","# Create empty lists to store images and labels\n","images = []\n","labels = []\n","\n","# Define a dictionary to map folder names to class labels\n","class_mapping = {\n","    'Age-related macular degeneration (ARMD )': 0,\n","    'Branch retinal vein occlusion(BRVO)': 1,\n","    'Central retinal vein occlusion (CRVO)': 2,\n","    'Cotton wool spots (CWS)': 3,\n","    'Central serous retinopathy (CSR)' : 4,\n","    'Exudative detachment of the retina (EDN)': 5,\n","    'Microaneurysms (MCA)': 6,\n","    'Optic disc edema (ODE)' : 7,\n","    'Posterior retinal hemorrhage (PRH)' : 8,\n","    'Retinal hemorrhages (HR)' : 9,\n","    'Tortuous vessels (TV)' : 10,\n","    'Vitreous hemorrhage ( VH )' : 11\n","\n","\n","}\n","\n","# Iterate through each folder in the root directory\n","for folder_name, class_label in class_mapping.items():\n","    folder_path = os.path.join(data_root, folder_name)\n","\n","    # Iterate through each image file in the folder\n","    for image_file in os.listdir(folder_path):\n","        if image_file.endswith('.jpg') or image_file.endswith('.jpeg') or image_file.endswith('.png'):\n","            image_path = os.path.join(folder_path, image_file)\n","\n","            # Load and preprocess the image\n","            img = Image.open(image_path)\n","            img = img.resize((224, 224))  # Resize to a suitable input size\n","            img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n","\n","            # Append the preprocessed image and its label to the lists\n","            images.append(img)\n","            labels.append(class_label)\n","\n","# Convert the lists to NumPy arrays\n","images = np.array(images)\n","labels = np.array(labels)\n"]},{"cell_type":"code","execution_count":null,"id":"3b966054","metadata":{"id":"3b966054","outputId":"3e7b3071-0901-41f6-bebe-98372ba01cc0"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-02-27 16:09:58.463264: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-02-27 16:09:58.483598: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-02-27 16:09:58.574224: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-27 16:09:58.574288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-27 16:09:58.587747: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-27 16:09:58.623530: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-02-27 16:09:58.624655: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-02-27 16:09:59.243192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["Found 1186 images belonging to 12 classes.\n","Found 293 images belonging to 12 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87910968/87910968 [==============================] - 93s 1us/step\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","38/38 [==============================] - ETA: 0s - loss: 1.9727 - accuracy: 0.3870\n","Epoch 1: val_loss improved from inf to 1.40520, saving model to best_model.h5\n"]},{"name":"stderr","output_type":"stream","text":["/home/admin1/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["38/38 [==============================] - 57s 1s/step - loss: 1.9727 - accuracy: 0.3870 - val_loss: 1.4052 - val_accuracy: 0.5495\n","Epoch 2/20\n","38/38 [==============================] - ETA: 0s - loss: 1.3928 - accuracy: 0.5422\n","Epoch 2: val_loss improved from 1.40520 to 1.21896, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 1.3928 - accuracy: 0.5422 - val_loss: 1.2190 - val_accuracy: 0.6143\n","Epoch 3/20\n","38/38 [==============================] - ETA: 0s - loss: 1.1515 - accuracy: 0.6206\n","Epoch 3: val_loss did not improve from 1.21896\n","38/38 [==============================] - 51s 1s/step - loss: 1.1515 - accuracy: 0.6206 - val_loss: 1.2957 - val_accuracy: 0.5529\n","Epoch 4/20\n","38/38 [==============================] - ETA: 0s - loss: 1.0993 - accuracy: 0.6391\n","Epoch 4: val_loss improved from 1.21896 to 1.09332, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 1.0993 - accuracy: 0.6391 - val_loss: 1.0933 - val_accuracy: 0.6246\n","Epoch 5/20\n","38/38 [==============================] - ETA: 0s - loss: 0.9529 - accuracy: 0.6863\n","Epoch 5: val_loss improved from 1.09332 to 1.04970, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 0.9529 - accuracy: 0.6863 - val_loss: 1.0497 - val_accuracy: 0.6485\n","Epoch 6/20\n","38/38 [==============================] - ETA: 0s - loss: 0.8992 - accuracy: 0.6965\n","Epoch 6: val_loss improved from 1.04970 to 0.98024, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 0.8992 - accuracy: 0.6965 - val_loss: 0.9802 - val_accuracy: 0.6655\n","Epoch 7/20\n","38/38 [==============================] - ETA: 0s - loss: 0.7563 - accuracy: 0.7445\n","Epoch 7: val_loss improved from 0.98024 to 0.77553, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 0.7563 - accuracy: 0.7445 - val_loss: 0.7755 - val_accuracy: 0.7270\n","Epoch 8/20\n","38/38 [==============================] - ETA: 0s - loss: 0.7403 - accuracy: 0.7496\n","Epoch 8: val_loss did not improve from 0.77553\n","38/38 [==============================] - 51s 1s/step - loss: 0.7403 - accuracy: 0.7496 - val_loss: 0.8816 - val_accuracy: 0.6689\n","Epoch 9/20\n","38/38 [==============================] - ETA: 0s - loss: 0.7177 - accuracy: 0.7513\n","Epoch 9: val_loss did not improve from 0.77553\n","38/38 [==============================] - 51s 1s/step - loss: 0.7177 - accuracy: 0.7513 - val_loss: 0.9618 - val_accuracy: 0.6689\n","Epoch 10/20\n","38/38 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.7698\n","Epoch 10: val_loss did not improve from 0.77553\n","38/38 [==============================] - 52s 1s/step - loss: 0.7047 - accuracy: 0.7698 - val_loss: 0.9048 - val_accuracy: 0.7167\n","Epoch 11/20\n","38/38 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.7841\n","Epoch 11: val_loss did not improve from 0.77553\n","38/38 [==============================] - 52s 1s/step - loss: 0.6696 - accuracy: 0.7841 - val_loss: 0.7939 - val_accuracy: 0.7304\n","Epoch 12/20\n","38/38 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.7656\n","Epoch 12: val_loss did not improve from 0.77553\n","38/38 [==============================] - 52s 1s/step - loss: 0.6445 - accuracy: 0.7656 - val_loss: 0.8618 - val_accuracy: 0.7235\n","Epoch 13/20\n","38/38 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.7614\n","Epoch 13: val_loss did not improve from 0.77553\n","38/38 [==============================] - 52s 1s/step - loss: 0.7085 - accuracy: 0.7614 - val_loss: 0.8527 - val_accuracy: 0.7235\n","Epoch 14/20\n","38/38 [==============================] - ETA: 0s - loss: 0.6549 - accuracy: 0.7656\n","Epoch 14: val_loss improved from 0.77553 to 0.76949, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 0.6549 - accuracy: 0.7656 - val_loss: 0.7695 - val_accuracy: 0.7099\n","Epoch 15/20\n","38/38 [==============================] - ETA: 0s - loss: 0.5939 - accuracy: 0.8010\n","Epoch 15: val_loss did not improve from 0.76949\n","38/38 [==============================] - 52s 1s/step - loss: 0.5939 - accuracy: 0.8010 - val_loss: 0.8652 - val_accuracy: 0.7235\n","Epoch 16/20\n","38/38 [==============================] - ETA: 0s - loss: 0.5434 - accuracy: 0.8153\n","Epoch 16: val_loss improved from 0.76949 to 0.60529, saving model to best_model.h5\n","38/38 [==============================] - 52s 1s/step - loss: 0.5434 - accuracy: 0.8153 - val_loss: 0.6053 - val_accuracy: 0.8191\n","Epoch 17/20\n","38/38 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7985\n","Epoch 17: val_loss did not improve from 0.60529\n","38/38 [==============================] - 51s 1s/step - loss: 0.5927 - accuracy: 0.7985 - val_loss: 0.8011 - val_accuracy: 0.7338\n","Epoch 18/20\n","38/38 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.8331\n","Epoch 18: val_loss did not improve from 0.60529\n","38/38 [==============================] - 51s 1s/step - loss: 0.5248 - accuracy: 0.8331 - val_loss: 0.6884 - val_accuracy: 0.7577\n","Epoch 19/20\n","38/38 [==============================] - ETA: 0s - loss: 0.4849 - accuracy: 0.8440\n","Epoch 19: val_loss did not improve from 0.60529\n","38/38 [==============================] - 51s 1s/step - loss: 0.4849 - accuracy: 0.8440 - val_loss: 0.7391 - val_accuracy: 0.7270\n","Epoch 20/20\n","38/38 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.8229\n","Epoch 20: val_loss did not improve from 0.60529\n","38/38 [==============================] - 52s 1s/step - loss: 0.5362 - accuracy: 0.8229 - val_loss: 0.7198 - val_accuracy: 0.7782\n","Found 1479 images belonging to 12 classes.\n","47/47 [==============================] - 52s 1s/step - loss: 0.5397 - accuracy: 0.8127\n","Testing Accuracy: 0.8127112984657288\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import InceptionV3\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# Define your data directory\n","data_dir = '/home/admin1/Downloads/small datast'\n","\n","# Define image size and batch size\n","image_size = (224, 224)\n","batch_size = 32\n","\n","# Create data generators with data augmentation\n","datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.2  # Split data into training and validation sets\n",")\n","\n","train_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training'  # Use the training subset\n",")\n","\n","val_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation'  # Use the validation subset\n",")\n","\n","# Load the pre-trained GoogleNet (Inception V1) model without top (fully connected) layers\n","base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Add custom layers for your classification task\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation='relu')(x)\n","predictions = Dense(12, activation='softmax')(x)  # 12 output classes\n","\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Freeze the layers of the pre-trained model\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile the model\n","model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Define a callback to save the best model\n","checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    epochs=20,\n","    validation_data=val_generator,\n","    validation_steps=len(val_generator),\n","    callbacks=[checkpoint]\n",")\n","\n","# Evaluate the model on the test set\n","test_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n","print(\"Testing Accuracy:\", test_acc)\n","\n","# Save the model\n","model.save('InceptionV3.h5')"]},{"cell_type":"code","execution_count":null,"id":"04b448cc","metadata":{"id":"04b448cc"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}