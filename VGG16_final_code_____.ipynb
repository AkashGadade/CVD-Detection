{"cells":[{"cell_type":"markdown","id":"e98d5c43","metadata":{"id":"e98d5c43"},"source":["### Step 1: Data Preprocessing and Loading"]},{"cell_type":"code","execution_count":null,"id":"9e9e4b1b","metadata":{"id":"9e9e4b1b"},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","\n","# Define the path to the root directory where your data is stored\n","data_root = '/home/admin1/Downloads/Akash CJ/Dataset-20240111T132314Z-001/Dataset/Balanced dataset'\n","\n","# Create empty lists to store images and labels\n","images = []\n","labels = []\n","\n","# Define a dictionary to map folder names to class labels\n","class_mapping = {\n","    'Age-related macular degeneration (ARMD )': 0,\n","    'Branch retinal vein occlusion(BRVO)': 1,\n","    'Central retinal vein occlusion (CRVO)': 2,\n","    'Cotton wool spots (CWS)': 3,\n","    'Central serous retinopathy (CSR)' : 4,\n","    'Exudative detachment of the retina (EDN)': 5,\n","    'Microaneurysms (MCA)': 6,\n","    'Optic disc edema (ODE)' : 7,\n","    'Posterior retinal hemorrhage (PRH)' : 8,\n","    'Retinal hemorrhages (HR)' : 9,\n","    'Tortuous vessels (TV)' : 10,\n","    'Vitreous hemorrhage ( VH )' : 11\n","\n","\n","}\n","\n","# Iterate through each folder in the root directory\n","for folder_name, class_label in class_mapping.items():\n","    folder_path = os.path.join(data_root, folder_name)\n","\n","    # Iterate through each image file in the folder\n","    for image_file in os.listdir(folder_path):\n","        if image_file.endswith('.jpg') or image_file.endswith('.jpeg') or image_file.endswith('.png'):\n","            image_path = os.path.join(folder_path, image_file)\n","\n","            # Load and preprocess the image\n","            img = Image.open(image_path)\n","            img = img.resize((224, 224))  # Resize to a suitable input size\n","            img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n","\n","            # Append the preprocessed image and its label to the lists\n","            images.append(img)\n","            labels.append(class_label)\n","\n","# Convert the lists to NumPy arrays\n","images = np.array(images)\n","labels = np.array(labels)\n"]},{"cell_type":"markdown","id":"b15ce886","metadata":{"id":"b15ce886"},"source":["Explanation: In this step, we load and preprocess the dataset. We iterate through each class folder, read image files, resize them to a common size (224x224), and normalize the pixel values to the range [0, 1]. We also map folder names to class labels and store the images and labels in NumPy arrays."]},{"cell_type":"markdown","id":"dcd83b72","metadata":{"id":"dcd83b72"},"source":["### Step 2: Data Augmentation with Directory Structure (MobileNetV2)"]},{"cell_type":"code","execution_count":null,"id":"1e97c194","metadata":{"id":"1e97c194"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# Define your data directory\n","data_dir = '/home/admin1/Downloads/Akash CJ/Dataset-20240111T132314Z-001/Dataset/Balanced dataset'\n","\n","# Define image size and batch size\n","image_size = (224, 224)\n","batch_size = 32\n","\n","# Create data generators with data augmentation\n","datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    validation_split=0.2  # Split data into training and validation sets\n",")\n","\n","train_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='training'  # Use the training subset\n",")\n","\n","val_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    subset='validation'  # Use the validation subset\n",")\n","\n","# Load the pre-trained MobileNetV2 model without top (fully connected) layers\n","base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","# Add custom layers for your classification task\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation='relu')(x)\n","predictions = Dense(12, activation='softmax')(x)  # 12 output classes\n","\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","# Freeze the layers of the pre-trained model\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","# Compile the model\n","model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Define a callback to save the best model\n","checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n","\n","# Train the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(train_generator),\n","    epochs=20,\n","    validation_data=val_generator,\n","    validation_steps=len(val_generator),\n","    callbacks=[checkpoint]\n",")\n","\n","# Evaluate the model on the test set\n","test_generator = datagen.flow_from_directory(\n","    data_dir,\n","    target_size=image_size,\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator))\n","print(\"Testing Accuracy:\", test_acc)\n","\n","# Save the model\n","model.save('MobileNet.h5')\n"]},{"cell_type":"code","execution_count":null,"id":"0fafc149","metadata":{"id":"0fafc149"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9401d95a","metadata":{"id":"9401d95a"},"outputs":[],"source":["#pip install tensorflow"]},{"cell_type":"markdown","id":"4d2b18d2","metadata":{"id":"4d2b18d2"},"source":["### Step 3 : Visualizing the learning of MobileNetV2 Architecture"]},{"cell_type":"code","execution_count":null,"id":"00017b32","metadata":{"id":"00017b32"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Extract training history\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","train_acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","epochs = range(1, len(train_loss) + 1)\n","\n","# Plot training and validation loss\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, train_loss, 'r', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Plot training and validation accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, train_acc, 'r', label='Training Accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"NJwTXWBvkNUR","metadata":{"id":"NJwTXWBvkNUR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"544e1866","metadata":{"id":"544e1866"},"outputs":[],"source":["#pip install tensorflow"]},{"cell_type":"markdown","id":"f18fd944","metadata":{"id":"f18fd944"},"source":["### Step 4 : Testing"]},{"cell_type":"code","execution_count":null,"id":"b16731a3","metadata":{"id":"b16731a3"},"outputs":[],"source":["import os\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import load_model\n","import time\n","import matplotlib.pyplot as plt\n","\n","# Path to the folder containing your testing images\n","testing_dir = 'test/'\n","\n","# Load the best-trained model\n","best_model = load_model('MobileNet.h5')\n","\n","# Initialize an empty list to store the image file paths\n","image_paths = []\n","\n","# Initialize a set to keep track of processed images to avoid duplication\n","processed_images = set()\n","\n","# Define a threshold for classifying as CVD (you can adjust this threshold)\n","threshold = 5.986454840355e-07  # Adjust this value as needed\n","\n","while True:\n","    # Get a list of all image files in the testing directory\n","    for filename in os.listdir(testing_dir):\n","        if filename.endswith(\".png\") and filename not in processed_images:\n","            image_path = os.path.join(testing_dir, filename)\n","\n","            # Load and preprocess the image\n","            img = image.load_img(image_path, target_size=(224, 224))\n","            img = image.img_to_array(img)\n","            img = np.expand_dims(img, axis=0)\n","            img /= 255.0\n","\n","            # Make predictions for the image\n","            prediction = best_model.predict(img)\n","\n","            # Check if the predicted probability for CVD (index 2) is above the threshold\n","            is_cvd = prediction[0][2] <= threshold\n","            print(prediction[0][2])\n","\n","            # Display the image\n","            plt.imshow(img[0])\n","            plt.axis('off')  # Hide axes\n","            plt.show()\n","\n","            # Print the prediction result\n","            if is_cvd:\n","                print(f\"Image {filename}: Potential Risk of CVD .\")\n","            else:\n","                print(f\"Image {filename}: No signs of CVD.\")\n","\n","            # Add the filename to the set of processed images\n","            processed_images.add(filename)\n","\n","    # Sleep for a while to avoid continuously checking the folder\n","    time.sleep(5)  # Adjust the sleep duration as needed\n"]},{"cell_type":"code","execution_count":null,"id":"3e6d23ef","metadata":{"id":"3e6d23ef"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"_2F0hfeUgLEY","metadata":{"id":"_2F0hfeUgLEY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"oKT9MAYBgLHM","metadata":{"id":"oKT9MAYBgLHM"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}